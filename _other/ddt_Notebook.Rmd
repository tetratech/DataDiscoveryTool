---
title: "Notes on Data Discovery Tool QAQC Update"
output: html_notebook
---

Data Discovery Tool (ddt) created by USEPA for use with Water Quality Data Portal (v1.1.0.0000).

# Packages
A number of packages are included with the ddt and ship in the R portal version.

In order run as a stand alone version will need the following packages installed.
```{r, eval=FALSE}
# libraries to be installed
data.packages = c("assertthat"
                  , "base64enc"
                  , "chron"
                  , "data.table"
                  , "dataRetrieval"
                  , "DBI"
                  , "devtools"
                  , "dplyr"
                  , "DT"
                  , "ggplot2"
                  , "git2r"
                  , "htmlwidgets"
                  , "httpuv"
                  , "httr"
                  , "jsonlite"
                  , "lazyeval"
                  , "leaflet"
                  , "lubridate"
                  , "memoise"
                  , "openssl"
                  , "png"
                  , "R6"
                  , "raster"
                  #, "rCharts" #non-CRANN
                  , "readr"
                  , "rstudioapi"
                  , "scales"
                  , "shiny"
                  , "shinyBS"
                  , "sourcetools"
                  , "sp"
                  , "stringr"
                  , "tibble"
                  , "whisker"
                  , "withr"
                  , "xml2"
                  , "xtable"
                  , "XLconnect"
                  )
# install via lapply
lapply(data.packages,function(x) install.packages(x))
```

```{r, eval=FALSE}
# Install non-CRANN packages
require(devtools)
install_github("ramnathv/rCharts")

```

May need shinyFiles if loading and saving Queries and Filters becomes problematic.

# Warning Message
Get 3 warnings whether run the app from ui.R or server.R.  The "if" statement mentioned is not in 
the code. 
```{r, eval=FALSE}
Warning in if (getAttribs(panels[[i]])$value %in% open) { :
  the condition has length > 1 and only the first element will be used
Warning in if (getAttribs(panels[[i]])$value %in% open) { :
  the condition has length > 1 and only the first element will be used
Warning in if (getAttribs(panels[[i]])$value %in% open) { :
  the condition has length > 1 and only the first element will be used
```

# GitHub
For updates the code was pulled out of R Portal and run as stand alone code.  
This code was uploaded to GitHub for tracking purposes.

https://github.com/tetratech/DataDiscoveryTool

# Version
## R
Troubleshooting_v1.pdf (included in files) says that the R version is 3.2.0.

According to website v1.1.0.0000 uses R version 3.3.1 (for Mac assume is the same for PC).

Using v3.4.1 (32-bit) for development of modifications.

## Code
Started with v1.1.0.9000 on GitHub.  Each update increments the development number (9000).

To make use of RStudio's outlining features adding "Tt Mod" comments to each section where make changes in R files.  See example below.

```{r, eval=FALSE}
  ## Tt Mod, Check Data, Save Data 
  output$SaveData <- downloadHandler(
    filename = function() {
      strFile <- paste0("DDT_Data_",format(Sys.time(),"%Y%m%d_%H%M%S"),".rds")}
    ,content = function(file) {
      saveRDS(all_data(),file)
    }
  )
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```

No changes to other code will be made that are not necessary for the features that are being added to the tool.

# Test Data
Selection Parameters

* HUC = 04010301
* Char Group = Nutrient
* Site Type = Stream

wqGateway

* Results = 417,417 obs. of 65 variables
* Stations =    566 obs. of 36 variables

ddt, HUC only

* Results = 259,939
* Stations =  1,001

ddt, HUC, CharGroup, and SiteType

* Results = 9,423
* Stations =  199

WQP Web Service Query URL

https://www.waterqualitydata.us/Result/search?siteType=Stream&huc=04010301&characteristicType=Nutrient&mimeType=tsv&sorted=no


# Query Data

Add 2 buttons (Save Query and Load Query).

UpdateSelectize boxes not coming out right.  Need to add "choices" and "selected" to update string.  The boxes still work with original 'choices' after the update.

Overwrite contents with "NA" or "0".  "NULL" will not put anything in the box so it retains the current state.

Test different Query files.
```{r, eval=FALSE}
# check RDS
myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
myFiles <- list.files(path=myDir, pattern="DDT_Query_*")
myFiles
myList <- readRDS(file.path(myDir,myFiles[length(myFiles)])) # load the last one
#myList <- readRDS(file.path(myDir,myFiles[8]))
myList
myList$county
```

Need to check if empty and respond with NA or 0.

The "URL" updates when changes are made to the input boxes.

# View Data
Added filters for ActivityTypeCode, SampleCollectionEquipmentName, and ResultStatusID.

Need to hook into "filtered_data()".

Test different Filters files.
```{r, eval=FALSE}
# check RDS
myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
myFiles <- list.files(path=myDir, pattern="DDT_Filters_*")
myFiles
myList <- readRDS(file.path(myDir,myFiles[length(myFiles)])) # load the last one
myList
```

For update statements:

1. Modify "choices" to match original code??

2. Update Filters code with "NULL" where appropriate.

* Timing issue.  Is working but when click on header to expand the Filter different code runs.
To populate the box.

* Need to programatically expand the boxes when "apply" the filter.


# Check Data
1. Add button to load data.

2. Modify save data to use 2 files (Keep and Exclude data).  May want "metadata" file as well.

3. Non-Detects.  
Modify to use 1/2 MDL as default.  
Remove "remove option".  
Add DL_Lo and DL-Hi fields to the data.

Will most likely need to modify the structure of the "data" data frame.

Test different Data files.
```{r, eval=FALSE}
# check RDS
myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
myFiles <- list.files(path=myDir, pattern="DDT_Data_*")
myFiles
myData <- readRDS(file.path(myDir,myFiles[length(myFiles)])) # load the last one
#myData <- readRDS(file.path(myDir,myFiles[4]))
names(myData)
str(myData)
View(myData)
```

# Utah data
myData.rds <- readRDS(file.path(myDir,myFiles[7]))
load(file.path(myDir,myFiles[19]))
myData.rda <- temp_data



# Data Format
DDT uses importWQP() rather than readWQPdata().  The former returns only a data frame.  The later returns a sites/stations file as well.  They take different inputs (URL vs. parameters) and the help examples are different so have to construct similar data requests so can compare.
```{r, eval=FALSE}
# load library
library(dataRetrieval)

# "import" uses a URL
datarequest_import <- constructWQPURL('USGS-01594440','01075', '', '') 
                      # siteNumbers, parameterCd, startDate, endDate, zip=FALSE
data_import <- importWQP(datarequest_import)  # runtime = 10 seconds
dim(data_import) # 65 65
str(data_import) # Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	65 obs. of  65 variables:

# "read" uses parameters
datarequest_read <- c(siteid="USGS-01594440", USGSPCode="01075")
data_read <- readWQPdata(datarequest_read)    # runtime > 15 *minutes*
dim(data_read) # 45559 65
str(data_read)   # 2 tables ?
```
Different results and different run times.  importWQP is *much* faster so that is what is used to get "data" via the "IMPORT" button.  Uses getData.R; constructs a URL based on inputs on the Query Data tab.

The number of records prior on the same pop up is done through a call to the webservices URL and getting the info in the header.

```{r, eval=FALSE}
# get station and record info. 

# define URL
myURL <- "https://www.waterqualitydata.us/Result/search?statecode=US%3A55&siteType=Stream&huc=04010301&sampleMedia=Water&characteristicType=Nutrient&startDateLo=01-01-2000&startDateHi=12-31-2015&mimeType=tsv&sorted=no"

# show HEAD
HEAD(myURL)

# show headers
headers(HEAD(myURL))
```


## Add Detection Limit Fields

Add DL Hi and Lo

* DetectionLimitLo <- 0
* DetectionLimitHi <- DetectionQuantitationLimitMeasure.MeasureValue

```{r, eval=FALSE}
# load library
library(dataRetrieval)

# "import" uses a URL
datarequest_import <- constructWQPURL('USGS-01594440','01075', '', '') 
                      # siteNumbers, parameterCd, startDate, endDate, zip=FALSE
data_import <- importWQP(datarequest_import)  # runtime = 10 seconds
dim(data_import) # 65 65
str(data_import) # Cla

# Add DL Lo and Hi
data_import$DetectionLimit_Lo <- 0
data_import$DetectionLimit_Hi<- data_import$DetectionQuantitationLimitMeasure.MeasureValue



```

## Testing - data frames

###Output DATA (retval)
```{r, eval=FALSE}
# check RDS
myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
myFiles <- list.files(path=myDir, pattern="DDT_Data_retval_*")
myFiles
myData <- readRDS(file.path(myDir,myFiles[length(myFiles)])) # load the last one
str(myData)
View(myData)
```

###Output DATA (siteInfo)
```{r, eval=FALSE}
# check RDS
myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
myFiles <- list.files(path=myDir, pattern="DDT_Data_SiteInfo_*")
myFiles
myData <- readRDS(file.path(myDir,myFiles[length(myFiles)])) # load the last one
str(myData)
View(myData)
```

###Test Merge

Fields not modified in ddt yet so can use WQX names.
, by.x=c("Organization", "Station")
```{r, eval=FALSE}
myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
retval   <- readRDS(file.path(myDir,"DDT_Data_retval_20170802_105845.rds"))
siteInfo <- readRDS(file.path(myDir,"DDT_Data_SiteInfo_20170802_105845.rds"))

    MoreFlds <- c("OrganizationIdentifier", "MonitoringLocationIdentifier"
                  , "MonitoringLocationName", "LatitudeMeasure", "LongitudeMeasure"
                  , "HUCEightDigitCode", "huc8name"
                  , "StateName", "CountyName"
                  , "MonitoringLocationTypeName")
    
    MoreFlds %in% names(siteInfo)

    c("OrganizationIdentifier", "MonitoringLocationIdentifier") %in% names(siteInfo)
    c("OrganizationIdentifier", "MonitoringLocationIdentifier") %in% names(retval)
    
    MoreFlds %in% names(retval)
    

# merge
retval.merge <- merge(retval,siteInfo[,MoreFlds]
                    , by.x=c("OrganizationIdentifier", "MonitoringLocationIdentifier")
                    , by.y=c("OrganizationIdentifier", "MonitoringLocationIdentifier")
                    , all.x=TRUE, sort=FALSE)
# QC numbers
dim(retval)
dim(siteInfo)
dim(retval.merge)
# X + Y - 2by = total columns
ncol(retval) + ncol(siteInfo[,MoreFlds]) - 2 == ncol(retval.merge)

```


# getData.R
Saving output to check
```{r, eval=FALSE}
# # # # testing (temporary)
      myDateTime <- format(Sys.time(),"%Y%m%d_%H%M%S")
      #
      myFile <- file.path("C:","Users","Erik.Leppo","Downloads",paste0("DDT_Data_SiteInfo_",myDateTime))
      saveRDS(siteInfo,paste0(myFile,".rds"))
      write.csv(siteInfo,paste0(myFile,".csv"))
      #
      myFile <- file.path("C:","Users","Erik.Leppo","Downloads",paste0("DDT_Data_retval_",myDateTime))
      saveRDS(retval,paste0(myFile,".rds"))
      write.csv(retval,paste0(myFile,".csv"))
      #
      myFile <- file.path("C:","Users","Erik.Leppo","Downloads",paste0("DDT_Data_retval_MERGE_",myDateTime))
      saveRDS(retval.merge,paste0(myFile,".rds"))
      write.csv(retval.merge,paste0(myFile,".csv"))
```



###County Info
```{r, eval=FALSE}
myDir <- file.path("C:","Users","Erik.Leppo","OneDrive - Tetra Tech, Inc"
                   ,"MyDocs_OneDrive", "GitHub", "DataDiscoveryTool", "external")
myFile <- "Counties.csv"
#
counties <- read.csv(file.path(myDir,myFile),header=FALSE
                     ,colClasses=c("factor","integer","character","factor","factor"))
head(counties)
# Counties in ddt is not counties.csv but counties_dropdown.csv

myCountyInfo <- counties[,2:4]
names(myCountyInfo) <- c("StateCode","CountyCode","CountyName")
#
head(myCountyInfo)

### Merge County into SiteInfo
myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
siteInfo <- readRDS(file.path(myDir,"DDT_Data_SiteInfo_20170802_094326.rds"))
# remove last field
names(siteInfo)
siteInfo <- siteInfo[,-46]

MergeCounty <- merge(siteInfo, myCountyInfo
                     , by = c("StateCode","CountyCode")
                     , all.x=TRUE)
head(MergeCounty)



```

# Check data
data2
```{r, eval=FALSE}
myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
myFiles <- list.files(path=myDir, pattern="DDT_Data_*")
myFiles
myData   <- readRDS(file.path(myDir,myFiles[6]))
myData   <- readRDS(file.path(myDir,myFiles[length(myFiles)]))
names(myData)
dim(myData)
str(myData)


```

# check global environment
```{r, eval=FALSE}
ls(globalenv())

myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
myFiles <- list.files(path=myDir, pattern="DDT_IMAGE_*")
myFiles
myData <- load(file.path(myDir,myFiles[length(myFiles)])) 


```

# DDT_QAQC

```{r, eval=FALSE}
myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
myFiles <- list.files(path=myDir, pattern="DDT_QAQC_*")
myFiles
myData <- readRDS(file.path(myDir,myFiles[1])) 
dim(myData)
View(myData)


```

# Edit DataTable
## DT library
https://github.com/rstudio/DT/issues/28
```{r, eval=FALSE}
devtools::install_github('rstudio/DT@feature/editor')

library(shiny)
library(DT)
shinyApp(
  ui = fluidPage(
    DT::dataTableOutput('x1')
  ),
  server = function(input, output, session) {
    x = iris
    x$Date = Sys.time() + seq_len(nrow(x))
    output$x1 = DT::renderDataTable(x, selection = 'none', rownames = FALSE)
    
    proxy = dataTableProxy('x1')
    
    observeEvent(input$x1_cell_edit, {
      info = input$x1_cell_edit
      str(info)
      i = info$row
      j = info$col + 1
      v = info$value
      x[i, j] <<- DT:::coerceValue(v, x[i, j])
      replaceData(proxy, x, resetPaging = FALSE, rownames = FALSE)
    })
  }
)
```

Attempt with just my data file


```{r, eval=FALSE}
#devtools::install_github('rstudio/DT@feature/editor')

library(shiny)
library(DT)
library(XLConnect)


shinyApp(
  ui = fluidPage(
    DT::dataTableOutput('x1')
  ),
  server = function(input, output, session) {
    
    # load my File
    data_QAQC <- XLConnect::readWorksheetFromFile("external/DDT_QAQC_Default.xlsx"
                                              , sheet="Methods Table", startRow=6, header=TRUE)
    
    #x = data_QAQC

    output$x1 = DT::renderDataTable(data_QAQC, server=TRUE, selection = 'none', rownames = FALSE)
    
    proxy_dt_QAQC = dataTableProxy('x1')
    
    observeEvent(input$x1_cell_edit, {
      info = input$x1_cell_edit
      str(info)
      i = info$row
      j = info$col + 1
      v = info$value
      # Change Value "v" only IF column = 8 AND logical (T/F)
      if(j==8 & (toupper(v)=="FALSE" | toupper(v)=="TRUE")) {
        data_QAQC[i, j] <<- DT:::coerceValue(toupper(v), data_QAQC[i, j])
        replaceData(proxy_dt_QAQC, data_QAQC, resetPaging = FALSE, rownames = FALSE)
      }
      
    })
  }
)
```


Radio buttons
http://rstudio.github.io/DT/011-radio.html

Manipulate a table
https://yihui.shinyapps.io/DT-proxy/



## rhandsontable library
http://stla.github.io/stlapblog/posts/shiny_editTable.html
```{r, eval=FALSE}
library(rhandsontable)
library(shiny)
editTable <- function(DF, outdir=getwd(), outfilename="table"){
  ui <- shinyUI(fluidPage(
    
    titlePanel("Edit and save a table"),
    sidebarLayout(
      sidebarPanel(
        helpText("Shiny app based on an example given in the rhandsontable package.", 
                 "Right-click on the table to delete/insert rows.", 
                 "Double-click on a cell to edit"),
        
        wellPanel(
          h3("Table options"),
          radioButtons("useType", "Use Data Types", c("TRUE", "FALSE"))
        ),
        br(), 
        
        wellPanel(
          h3("Save table"), 
          div(class='row', 
              div(class="col-sm-6", 
                  actionButton("save", "Save")),
              div(class="col-sm-6",
                  radioButtons("fileType", "File type", c("ASCII", "RDS")))
          )
        )
        
      ),
      
      mainPanel(
        wellPanel(
          uiOutput("message", inline=TRUE)
        ),
        
        actionButton("cancel", "Cancel last action"),
        br(), br(),
        
        rHandsontableOutput("hot"),
        br(),
        
        wellPanel(
          h3("Add a column"),
          div(class='row',
              div(class="col-sm-5",
                  uiOutput("ui_newcolname"),
                  actionButton("addcolumn", "Add")),
              div(class="col-sm-4",
                  radioButtons("newcolumntype", "Type", c("integer", "double", "character"))),
              div(class="col-sm-3")
          )
        )
        
      )
    )
  ))
  
  server <- shinyServer(function(input, output) {
    
    values <- reactiveValues()
    
    ## Handsontable
    observe({
      if (!is.null(input$hot)) {
        values[["previous"]] <- isolate(values[["DF"]])
        DF = hot_to_r(input$hot)
      } else {
        if (is.null(values[["DF"]]))
          DF <- DF
        else
          DF <- values[["DF"]]
      }
      values[["DF"]] <- DF
    })
    
    output$hot <- renderRHandsontable({
      DF <- values[["DF"]]
      if (!is.null(DF))
        rhandsontable(DF, useTypes = as.logical(input$useType), stretchH = "all")
    })
    
    ## Save 
    observeEvent(input$save, {
      fileType <- isolate(input$fileType)
      finalDF <- isolate(values[["DF"]])
      if(fileType == "ASCII"){
        dput(finalDF, file=file.path(outdir, sprintf("%s.txt", outfilename)))
      }
      else{
        saveRDS(finalDF, file=file.path(outdir, sprintf("%s.rds", outfilename)))
      }
    }
    )
    
     ## Cancel last action
     observeEvent(input$cancel, {
       if(!is.null(isolate(values[["previous"]]))) values[["DF"]] <- isolate(values[["previous"]])
     })

     ## Add column
     output$ui_newcolname <- renderUI({
       textInput("newcolumnname", "Name", sprintf("newcol%s", 1+ncol(values[["DF"]])))
     })
     observeEvent(input$addcolumn, {
       DF <- isolate(values[["DF"]])
       values[["previous"]] <- DF
       newcolumn <- eval(parse(text=sprintf('%s(nrow(DF))', isolate(input$newcolumntype))))
       values[["DF"]] <- setNames(cbind(DF, newcolumn, stringsAsFactors=FALSE), c(names(DF), isolate(input$newcolumnname)))
     })
    
    ## Message
    output$message <- renderUI({
      if(input$save==0){
        helpText(sprintf("This table will be saved in folder \"%s\" once you press the Save button.", outdir))
      }else{
        outfile <- ifelse(isolate(input$fileType)=="ASCII", "table.txt", "table.rds")
        fun <- ifelse(isolate(input$fileType)=="ASCII", "dget", "readRDS")
        list(helpText(sprintf("File saved: \"%s\".", file.path(outdir, outfile))),
             helpText(sprintf("Type %s(\"%s\") to get it.", fun, outfile)))
      }
    })
    
  })
  
  ## run app 
  runApp(list(ui=ui, server=server))
  return(invisible())
}


# Create DF
( DF <- data.frame(Value = 1:10, Status = TRUE, Name = LETTERS[1:10],
                 Date = seq(from = Sys.Date(), by = "days", length.out = 10),
                 stringsAsFactors = FALSE) )

# Run the App
editTable(DF, outdir="~/Documents/", outfilename="newDF")

```
### slim example
http://jrowen.github.io/rhandsontable/
(will show in "viewer" in RStudio)
Did not use this method.  Stayed with DT library.
```{r, eval=FALSE}
DF = data.frame(val = 1:10, bool = TRUE, big = LETTERS[1:10],
                small = letters[1:10],
                dt = seq(from = Sys.Date(), by = "days", length.out = 10),
                stringsAsFactors = FALSE)

rhandsontable(DF, readOnly = TRUE, width = 550, height = 300) %>%
  hot_cols(columnSorting = TRUE) %>% 
    hot_col("bool", readOnly = FALSE) %>%
      hot_context_menu(allowRowEdit = FALSE, allowColEdit = FALSE)


```

# QAQC, Advanced
## Parameter Combos
ddt version replace myData with all_data()
```{r, eval=FALSE}
# 0. get a data file to use as an example
myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
myFiles <- list.files(path=myDir, pattern="DDT_Data_*")
myFiles
myData   <- readRDS(file.path(myDir,myFiles[5]))
# 1. get all combos, include number and min/max
myFields <- c("ActivityMediaName", "CharacteristicName", "ResultSampleFractionText"
              , "USGSPCode", "Unit", "Result")
myData4QAQC <- myData[,myFields]


myData.QAQC <- unique(myData4QAQC[,-6])

# summarize #library(dplyr)
myData.QAQC.Summary <- myData4QAQC %>%
                        group_by(ActivityMediaName, CharacteristicName, ResultSampleFractionText, USGSPCode, Unit) %>%
                          summarise(n=n(),min=min(Result,na.rm=TRUE),max=max(Result,na.rm=TRUE))
View(myData.QAQC.Summary)

#table(myData[,myFields[-6]],myData[,myFields[6]])


# 2. mark as "new"

x <- merge(myData.QAQC.Summary, data_QAQC[c(myFields, "Apply.QAQC")], by=myFields, all.x=TRUE)




```

```{r, eval=FALSE}
myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
myFiles <- list.files(path=myDir, pattern=" *.tsv")
myFiles
myData   <- read.delim(file.path(myDir,myFiles[1]),skip=10)
# read only some lines
myData.Meta <- read.delim(file.path(myDir,myFiles[1]),nrows=9, header=FALSE
                          , col.names=c("Meta.Field","Meta.Value"))
myData.URL <- as.character(myData.Meta[3,2])

```

# Summary Plots and Tables
Test out summary plots and tables of filtered data.
20170925.  Change from RDS to RDA.
```{r, eval=FALSE}
# library
require(dplyr)
# import data.  This will take the place of "filtered_data()"
myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
myFiles <- list.files(path=myDir, pattern="DDT_Data_R8 *")
myFiles
myFile <- myFiles[6] #Utah
#myData <- readRDS(file.path(myDir,myFile)) #~10 seconds for biggest file
load(file.path(myDir,myFiles[6])) #temp_data and temp_url_display
myData <- temp_data
# Add Year
myData$Year <- as.numeric(format(myData$ActivityStartDate, "%Y"))
# summarize by site


# summarize #library(dplyr)

summarize_Site <- myData %>%
                    group_by(MonitoringLocationIdentifier) %>%
                      summarise(nResults=n(),begYear=min(Year,na.rm=TRUE)
                                ,endYear=max(Year,na.rm=TRUE))
#View(summarize_Site)
# SiteID and ActID (double summarize with pipes)
summarize_Site_nAct <- myData %>%
                          group_by(MonitoringLocationIdentifier, ActivityIdentifier) %>%
                            summarise(n=n()) %>% 
                              group_by(MonitoringLocationIdentifier) %>%
                                summarise(nActivities=n())
#View(summarize_Site_nAct)
# SiteID n(Samples)
summarize_Site_nSamps <- myData %>%
                            group_by(MonitoringLocationIdentifier
                                    , ActivityStartDate
                                    , ActivityStartTime.Time
                                    , ActivityDepthHeightMeasure.MeasureValue) %>% 
                              summarize(n=n()) %>%
                                group_by(MonitoringLocationIdentifier) %>%
                                  summarize(nSamples=n())
#View(summarize_Site_nSamps)
# SiteID n(Parameters)
summarize_Site_nParam <- myData %>%
                          group_by(MonitoringLocationIdentifier, CharacteristicName) %>%
                            summarize(n=n()) %>%
                              group_by(MonitoringLocationIdentifier) %>%
                                summarize(nParameters=n())
#View(summarize_Site_nParam)
# Create single table
Site_Summary <- merge(summarize_Site, summarize_Site_nAct, by="MonitoringLocationIdentifier")
Site_Summary <- merge(Site_Summary, summarize_Site_nSamps, by="MonitoringLocationIdentifier")
Site_Summary <- merge(Site_Summary, summarize_Site_nParam, by="MonitoringLocationIdentifier")
View(Site_Summary)




# Plots
data_plot <- Site_Summary
  ## Plot by Site
  par(mfrow = c(2, 2))
  with(data_plot, plot.ecdf(nResults, ylab="CDF(x)", xlab=""
                            , main="A) Results by Site"
                            , panel.first=grid(lty=3)))
  with(data_plot, plot.ecdf(nActivities, ylab="CDF(x)", xlab=""
                            , main="B) Activities by Site"
                            , panel.first=grid(lty=3)))
  with(data_plot, plot.ecdf(nSamples, ylab="CDF(x)", xlab=""
                            , main="C) Samples by Site*"
                            , panel.first=grid(lty=3)))
  with(data_plot, plot.ecdf(nParameters, ylab="CDF(x)", xlab=""
                            , main="D) Parameters by Site"
                            , panel.first=grid(lty=3)))
  ## Plot by Years
  par(mfrow = c(2, 2))
  with(data_plot, plot.ecdf(begYear, ylab="CDF(x)", xlab=""
                            , main="E) Begin Year by Site"
                            , panel.first=grid(lty=3)))
  with(data_plot, plot.ecdf(endYear, ylab="CDF(x)", xlab=""
                            , main="F) End Year by Site"
                            , panel.first=grid(lty=3)))
  with(data_plot, plot.ecdf((endYear-begYear+1), ylab="CDF(x)", xlab=""
                            , main="G) Num Years by Site"
                            , panel.first=grid(lty=3)))
  with(data_plot, plot(begYear, endYear, ylab="End Year", xlab="Begin Year"
                       , main="H) End vs. Beg. Year by Site"
                       , panel.first=grid(lty=3)))
# Tables
  #~~~~~~~~~~~~~~
  #UT tribal data is of poor quality.  Numbers won't match between tables.
  # Some stations in more than one county.  AND more than one Type.
  #~~~~~~~~~~~~~~
  table_Sites_County <- myData %>%
                          group_by(StateName
                                  , CountyName
                                  , MonitoringLocationIdentifier) %>% 
                              summarize(nResults=n()) %>%
                                group_by(StateName, CountyName) %>%
                                  summarize(nSites=n())
  table_Site_County_MonLocType <- myData %>%
                              group_by(StateName
                                      , CountyName
                                      , MonitoringLocationTypeName
                                      , MonitoringLocationIdentifier) %>% 
                                  summarize(nResults=n()) %>%
                                    group_by(StateName
                                            , CountyName
                                            , MonitoringLocationTypeName) %>%
                                      summarize(nSites=n())
  # need wide format
  
# x2<-data.frame(with(myData, table(StateName, CountyName, MonitoringLocationTypeName)))
#   x2 <- reshape (x2, v.names=c("Freq"), idvar=c("StateName","CountyName"),
#                  timevar=c("MonitoringLocationTypeName"), drop=c(""), direction = "wide")

```



# OTHER
__Validate values__

validator = "
           function (value, callback) {
            setTimeout(function(){
              callback(value != TRUE);
            }, 1000)
           }"

# Style Questions

1. Buttons for Query and View inside or outside of "wells" (gray outline box).  For now have one of each so can see differences.  Need to pick one for the final.