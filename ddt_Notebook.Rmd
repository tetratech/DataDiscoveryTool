---
title: "Notes on Data Discovery Tool QAQC Update"
output: html_notebook
---

Data Discovery Tool (ddt) created by USEPA for use with Water Quality Data Portal (v1.1.0.0000).

# Packages
A number of packages are included with the ddt and ship in the R portal version.

In order run as a stand alone version will need the following packages installed.
```{r, eval=FALSE}
# libraries to be installed
data.packages = c("assertthat"
                  , "base64enc"
                  , "chron"
                  , "data.table"
                  , "dataRetrieval"
                  , "DBI"
                  , "devtools"
                  , "dplyr"
                  , "DT"
                  , "ggplot2"
                  , "git2r"
                  , "htmlwidgets"
                  , "httpuv"
                  , "httr"
                  , "jsonlite"
                  , "lazyeval"
                  , "leaflet"
                  , "lubridate"
                  , "memoise"
                  , "openssl"
                  , "png"
                  , "R6"
                  , "raster"
                  #, "rCharts" #non-CRANN
                  , "readr"
                  , "rstudioapi"
                  , "scales"
                  , "shiny"
                  , "shinyBS"
                  , "sourcetools"
                  , "sp"
                  , "stringr"
                  , "tibble"
                  , "whisker"
                  , "withr"
                  , "xml2"
                  , "xtable"
                  )
# install via lapply
lapply(data.packages,function(x) install.packages(x))
```

```{r, eval=FALSE}
# Install non-CRANN packages
require(devtools)
install_github("ramnathv/rCharts")

```

May need shinyFiles if loading and saving Queries and Filters becomes problematic.

# Warning Message
Get 3 warnings whether run the app from ui.R or server.R.  The "if" statement mentioned is not in 
the code. 
```{r, eval=FALSE}
Warning in if (getAttribs(panels[[i]])$value %in% open) { :
  the condition has length > 1 and only the first element will be used
Warning in if (getAttribs(panels[[i]])$value %in% open) { :
  the condition has length > 1 and only the first element will be used
Warning in if (getAttribs(panels[[i]])$value %in% open) { :
  the condition has length > 1 and only the first element will be used
```

# GitHub
For updates the code was pulled out of R Portal and run as stand alone code.  
This code was uploaded to GitHub for tracking purposes.

https://github.com/tetratech/DataDiscoveryTool

# Version
## R
Troubleshooting_v1.pdf (included in files) says that the R version is 3.2.0.

According to website v1.1.0.0000 uses R version 3.3.1 (for Mac assume is the same for PC).

Using v3.4.1 (32-bit) for development of modifications.

## Code
Started with v1.1.0.9000 on GitHub.  Each update increments the development number (9000).

To make use of RStudio's outlining features adding "Tt Mod" comments to each section where make changes in R files.  See example below.

```{r, eval=FALSE}
  ## Tt Mod, Check Data, Save Data 
  output$SaveData <- downloadHandler(
    filename = function() {
      strFile <- paste0("DDT_Data_",format(Sys.time(),"%Y%m%d_%H%M%S"),".rds")}
    ,content = function(file) {
      saveRDS(all_data(),file)
    }
  )
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```

No changes to other code will be made that are not necessary for the features that are being added to the tool.

# Test Data
Selection Parameters

* HUC = 04010301
* Char Group = Nutrient
* Site Type = Stream

wqGateway

* Results = 417,417 obs. of 65 variables
* Stations =    566 obs. of 36 variables

ddt, HUC only

* Results = 259,939
* Stations =  1,001

ddt, HUC, CharGroup, and SiteType

* Results = 9,423
* Stations =  199

WQP Web Service Query URL

https://www.waterqualitydata.us/Result/search?siteType=Stream&huc=04010301&characteristicType=Nutrient&mimeType=tsv&sorted=no


# Query Data

Add 2 buttons (Save Query and Load Query).

UpdateSelectize boxes not coming out right.  Need to add "choices" and "selected" to update string.  The boxes still work with original 'choices' after the update.

Overwrite contents with "NA" or "0".  "NULL" will not put anything in the box so it retains the current state.

Test different Query files.
```{r, eval=FALSE}
# check RDS
myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
myFiles <- list.files(path=myDir, pattern="DDT_Query_*")
myFiles
myList <- readRDS(file.path(myDir,myFiles[length(myFiles)])) # load the last one
#myList <- readRDS(file.path(myDir,myFiles[8]))
myList
myList$county
```

Need to check if empty and respond with NA or 0.

The "URL" updates when changes are made to the input boxes.

# View Data
Added filters for ActivityTypeCode, SampleCollectionEquipmentName, and ResultStatusID.

Need to hook into "filtered_data()".

Test different Filters files.
```{r, eval=FALSE}
# check RDS
myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
myFiles <- list.files(path=myDir, pattern="DDT_Filters_*")
myFiles
myList <- readRDS(file.path(myDir,myFiles[length(myFiles)])) # load the last one
myList
```

For update statements:

1. Modify "choices" to match original code??

2. Update Filters code with "NULL" where appropriate.

* Timing issue.  Is working but when click on header to expand the Filter different code runs.
To populate the box.

* Need to programatically expand the boxes when "apply" the filter.


# Check Data
1. Add button to load data.

2. Modify save data to use 2 files (Keep and Exclude data).  May want "metadata" file as well.

3. Non-Detects.  
Modify to use 1/2 MDL as default.  
Remove "remove option".  
Add DL_Lo and DL-Hi fields to the data.

Will most likely need to modify the structure of the "data" data frame.

Test different Data files.
```{r, eval=FALSE}
# check RDS
myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
myFiles <- list.files(path=myDir, pattern="DDT_Data_*")
myFiles
myData <- readRDS(file.path(myDir,myFiles[length(myFiles)])) # load the last one
#myData <- readRDS(file.path(myDir,myFiles[4]))
names(myData)
str(myData)
View(myData)
```

# Data Format
DDT uses importWQP() rather than readWQPdata().  The former returns only a data frame.  The later returns a sites/stations file as well.  They take different inputs (URL vs. parameters) and the help examples are different so have to construct similar data requests so can compare.
```{r, eval=FALSE}
# load library
library(dataRetrieval)

# "import" uses a URL
datarequest_import <- constructWQPURL('USGS-01594440','01075', '', '') 
                      # siteNumbers, parameterCd, startDate, endDate, zip=FALSE
data_import <- importWQP(datarequest_import)  # runtime = 10 seconds
dim(data_import) # 65 65
str(data_import) # Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	65 obs. of  65 variables:

# "read" uses parameters
datarequest_read <- c(siteid="USGS-01594440", USGSPCode="01075")
data_read <- readWQPdata(datarequest_read)    # runtime > 15 *minutes*
dim(data_read) # 45559 65
str(data_read)   # 2 tables ?
```
Different results and different run times.  importWQP is *much* faster so that is what is used to get "data" via the "IMPORT" button.  Uses getData.R; constructs a URL based on inputs on the Query Data tab.

The number of records prior on the same pop up is done through a call to the webservices URL and getting the info in the header.

```{r, eval=FALSE}
# get station and record info. 

# define URL
myURL <- "https://www.waterqualitydata.us/Result/search?statecode=US%3A55&siteType=Stream&huc=04010301&sampleMedia=Water&characteristicType=Nutrient&startDateLo=01-01-2000&startDateHi=12-31-2015&mimeType=tsv&sorted=no"

# show HEAD
HEAD(myURL)

# show headers
headers(HEAD(myURL))
```


## Add Detection Limit Fields

Add DL Hi and Lo

* DetectionLimitLo <- 0
* DetectionLimitHi <- DetectionQuantitationLimitMeasure.MeasureValue

```{r, eval=FALSE}
# load library
library(dataRetrieval)

# "import" uses a URL
datarequest_import <- constructWQPURL('USGS-01594440','01075', '', '') 
                      # siteNumbers, parameterCd, startDate, endDate, zip=FALSE
data_import <- importWQP(datarequest_import)  # runtime = 10 seconds
dim(data_import) # 65 65
str(data_import) # Cla

# Add DL Lo and Hi
data_import$DetectionLimit_Lo <- 0
data_import$DetectionLimit_Hi<- data_import$DetectionQuantitationLimitMeasure.MeasureValue



```

## Testing - data frames

###Output DATA (retval)
```{r, eval=FALSE}
# check RDS
myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
myFiles <- list.files(path=myDir, pattern="DDT_Data_retval_*")
myFiles
myData <- readRDS(file.path(myDir,myFiles[length(myFiles)])) # load the last one
str(myData)
View(myData)
```

###Output DATA (siteInfo)
```{r, eval=FALSE}
# check RDS
myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
myFiles <- list.files(path=myDir, pattern="DDT_Data_SiteInfo_*")
myFiles
myData <- readRDS(file.path(myDir,myFiles[length(myFiles)])) # load the last one
str(myData)
View(myData)
```

###Test Merge

Fields not modified in ddt yet so can use WQX names.
, by.x=c("Organization", "Station")
```{r, eval=FALSE}
myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
retval   <- readRDS(file.path(myDir,"DDT_Data_retval_20170802_105845.rds"))
siteInfo <- readRDS(file.path(myDir,"DDT_Data_SiteInfo_20170802_105845.rds"))

    MoreFlds <- c("OrganizationIdentifier", "MonitoringLocationIdentifier"
                  , "MonitoringLocationName", "LatitudeMeasure", "LongitudeMeasure"
                  , "HUCEightDigitCode", "huc8name"
                  , "StateName", "CountyName"
                  , "MonitoringLocationTypeName")
    
    MoreFlds %in% names(siteInfo)

    c("OrganizationIdentifier", "MonitoringLocationIdentifier") %in% names(siteInfo)
    c("OrganizationIdentifier", "MonitoringLocationIdentifier") %in% names(retval)
    
    MoreFlds %in% names(retval)
    

# merge
retval.merge <- merge(retval,siteInfo[,MoreFlds]
                    , by.x=c("OrganizationIdentifier", "MonitoringLocationIdentifier")
                    , by.y=c("OrganizationIdentifier", "MonitoringLocationIdentifier")
                    , all.x=TRUE, sort=FALSE)
# QC numbers
dim(retval)
dim(siteInfo)
dim(retval.merge)
# X + Y - 2by = total columns
ncol(retval) + ncol(siteInfo[,MoreFlds]) - 2 == ncol(retval.merge)

```


# getData.R
Saving output to check
```{r, eval=FALSE}
# # # # testing (temporary)
      myDateTime <- format(Sys.time(),"%Y%m%d_%H%M%S")
      #
      myFile <- file.path("C:","Users","Erik.Leppo","Downloads",paste0("DDT_Data_SiteInfo_",myDateTime))
      saveRDS(siteInfo,paste0(myFile,".rds"))
      write.csv(siteInfo,paste0(myFile,".csv"))
      #
      myFile <- file.path("C:","Users","Erik.Leppo","Downloads",paste0("DDT_Data_retval_",myDateTime))
      saveRDS(retval,paste0(myFile,".rds"))
      write.csv(retval,paste0(myFile,".csv"))
      #
      myFile <- file.path("C:","Users","Erik.Leppo","Downloads",paste0("DDT_Data_retval_MERGE_",myDateTime))
      saveRDS(retval.merge,paste0(myFile,".rds"))
      write.csv(retval.merge,paste0(myFile,".csv"))
```



###County Info
```{r, eval=FALSE}
myDir <- file.path("C:","Users","Erik.Leppo","OneDrive - Tetra Tech, Inc"
                   ,"MyDocs_OneDrive", "GitHub", "DataDiscoveryTool", "external")
myFile <- "Counties.csv"
#
counties <- read.csv(file.path(myDir,myFile),header=FALSE
                     ,colClasses=c("factor","integer","character","factor","factor"))
head(counties)
# Counties in ddt is not counties.csv but counties_dropdown.csv

myCountyInfo <- counties[,2:4]
names(myCountyInfo) <- c("StateCode","CountyCode","CountyName")
#
head(myCountyInfo)

### Merge County into SiteInfo
myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
siteInfo <- readRDS(file.path(myDir,"DDT_Data_SiteInfo_20170802_094326.rds"))
# remove last field
names(siteInfo)
siteInfo <- siteInfo[,-46]

MergeCounty <- merge(siteInfo, myCountyInfo
                     , by = c("StateCode","CountyCode")
                     , all.x=TRUE)
head(MergeCounty)



```

# Check data
data2
```{r}
myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
data2   <- readRDS(file.path(myDir,"DDT_Data_data2_20170802_140407.rds"))
names(data2)

x <- c("")

```

# check global environment
```{r, eval=FALSE}
ls(globalenv())

myDir <- file.path("C:","Users","Erik.Leppo","Downloads")
myFiles <- list.files(path=myDir, pattern="DDT_IMAGE_*")
myFiles
myData <- load(file.path(myDir,myFiles[length(myFiles)])) 


```


# Style Questions

1. Buttons for Query and View inside or outside of "wells" (gray outline box).  For now have one of each so can see differences.  Need to pick one for the final.
